{
  SQLContext sqlc=getSparkInterpreter().getSQLContext();
  SparkContext sc=sqlc.sparkContext();
  sc.setJobGroup(jobGroup,"Zeppelin",false);
  SchemaRDD rdd;
  Row[] rows=null;
  try {
    rdd=sqlc.sql(st);
    rows=rdd.take(maxResult + 1);
  }
 catch (  Exception e) {
    logger.error("Error",e);
    sc.clearJobGroup();
    return new InterpreterResult(Code.ERROR,e.getMessage());
  }
  String msg=null;
  List<Attribute> columns=scala.collection.JavaConverters.asJavaListConverter(rdd.queryExecution().analyzed().output()).asJava();
  for (  Attribute col : columns) {
    if (msg == null) {
      msg=col.name();
    }
 else {
      msg+="\t" + col.name();
    }
  }
  msg+="\n";
  int numRows=0;
  for (int r=0; r < maxResult && r < rows.length; r++) {
    Row row=rows[r];
    for (int i=0; i < columns.size(); i++) {
      String type=columns.get(i).dataType().toString();
      if ("BooleanType".equals(type)) {
        msg+=row.getBoolean(i);
      }
 else       if ("DecimalType".equals(type)) {
        msg+=row.getInt(i);
      }
 else       if ("DoubleType".equals(type)) {
        msg+=row.getDouble(i);
      }
 else       if ("FloatType".equals(type)) {
        msg+=row.getFloat(i);
      }
 else       if ("LongType".equals(type)) {
        msg+=row.getLong(i);
      }
 else       if ("IntegerType".equals(type)) {
        msg+=row.getInt(i);
      }
 else       if ("ShortType".equals(type)) {
        msg+=row.getShort(i);
      }
 else       if ("StringType".equals(type)) {
        msg+=row.getString(i);
      }
 else {
        msg+=row.getString(i);
      }
      if (i != columns.size() - 1) {
        msg+="\t";
      }
    }
    msg+="\n";
  }
  if (rows.length > maxResult) {
    msg+="\n<font color=red>Results are limited by " + maxResult + ".</font>";
  }
  InterpreterResult rett=new InterpreterResult(Code.SUCCESS,"%table " + msg);
  sc.clearJobGroup();
  return rett;
}
