{
  Settings settings=new Settings();
  settings.classpath().value_$eq(System.getProperty("java.class.path"));
  this.interpreter=new SparkILoop(null,new PrintWriter(out));
  interpreter.settings_$eq(settings);
  interpreter.createInterpreter();
  intp=interpreter.intp();
  intp.initializeSynchronous();
synchronized (sparkContextCreationLock) {
    intp.interpret("@transient var _binder = new java.util.HashMap[String, Object]()");
    Map<String,Object> binder=(Map<String,Object>)getValue("_binder");
    binder.put("out",new PrintStream(out));
    intp.interpret("System.setOut(_binder.get(\"out\").asInstanceOf[java.io.PrintStream])");
    intp.interpret("Console.setOut(_binder.get(\"out\").asInstanceOf[java.io.PrintStream])");
    intp.interpret("@transient val sc = com.nflabs.zeppelin.spark.SparkRepl.interpreter.createSparkContext()\n");
    intp.interpret("import org.apache.spark.SparkContext._");
    intp.interpret("val sqlc = new org.apache.spark.sql.SQLContext(sc)");
    intp.interpret("import sqlc.createSchemaRDD");
    sc=(SparkContext)getValue("sc");
  }
}
