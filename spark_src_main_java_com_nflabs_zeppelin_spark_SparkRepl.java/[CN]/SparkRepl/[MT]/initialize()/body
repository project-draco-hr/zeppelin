{
  Settings settings=new Settings();
  settings.classpath().value_$eq(System.getProperty("java.class.path"));
  PrintStream printStream=new PrintStream(out);
  this.interpreter=new SparkILoop(null,new PrintWriter(out));
  interpreter.settings_$eq(settings);
  interpreter.createInterpreter();
  intp=interpreter.intp();
  intp.initializeSynchronous();
  sc=createSparkContext();
  sqlc=new SQLContext(sc);
synchronized (sparkContextCreationLock) {
    intp.interpret("@transient var _binder = new java.util.HashMap[String, Object]()");
    Map<String,Object> binder=(Map<String,Object>)getValue("_binder");
    binder.put("out",printStream);
    binder.put("sc",sc);
    binder.put("sqlc",sqlc);
    intp.interpret("@transient var sc = _binder.get(\"sc\")");
    intp.interpret("import org.apache.spark.SparkContext._");
    intp.interpret("val sqlc = _binder.get(\"sqlc\")");
    intp.interpret("import sqlc.createSchemaRDD");
  }
}
