{
  URL[] urls=getClassloaderUrls();
  Settings settings=new Settings();
  if (getProperty("args") != null) {
    String[] argsArray=getProperty("args").split(" ");
    LinkedList<String> argList=new LinkedList<String>();
    for (    String arg : argsArray) {
      argList.add(arg);
    }
    SparkCommandLine command=new SparkCommandLine(scala.collection.JavaConversions.asScalaBuffer(argList).toList());
    settings=command.settings();
  }
  PathSetting pathSettings=settings.classpath();
  String classpath="";
  List<File> paths=currentClassPath();
  for (  File f : paths) {
    if (classpath.length() > 0) {
      classpath+=File.pathSeparator;
    }
    classpath+=f.getAbsolutePath();
  }
  if (urls != null) {
    for (    URL u : urls) {
      if (classpath.length() > 0) {
        classpath+=File.pathSeparator;
      }
      classpath+=u.getFile();
    }
  }
  DepInterpreter depInterpreter=getDepInterpreter();
  if (depInterpreter != null) {
    DependencyContext depc=depInterpreter.getDependencyContext();
    if (depc != null) {
      List<File> files=depc.getFiles();
      if (files != null) {
        for (        File f : files) {
          if (classpath.length() > 0) {
            classpath+=File.pathSeparator;
          }
          classpath+=f.getAbsolutePath();
        }
      }
    }
  }
  pathSettings.v_$eq(classpath);
  settings.scala$tools$nsc$settings$ScalaSettings$_setter_$classpath_$eq(pathSettings);
  settings.explicitParentLoader_$eq(new Some<ClassLoader>(Thread.currentThread().getContextClassLoader()));
  BooleanSetting b=(BooleanSetting)settings.usejavacp();
  b.v_$eq(true);
  settings.scala$tools$nsc$settings$StandardScalaSettings$_setter_$usejavacp_$eq(b);
  PrintStream printStream=new PrintStream(out);
  this.interpreter=new SparkILoop(null,new PrintWriter(out));
  interpreter.settings_$eq(settings);
  interpreter.createInterpreter();
  intp=interpreter.intp();
  intp.setContextClassLoader();
  intp.initializeSynchronous();
  completor=new SparkJLineCompletion(intp);
  sc=getSparkContext();
  if (sc.getPoolForName("fair").isEmpty()) {
    Value schedulingMode=org.apache.spark.scheduler.SchedulingMode.FAIR();
    int minimumShare=0;
    int weight=1;
    Pool pool=new Pool("fair",schedulingMode,minimumShare,weight);
    sc.taskScheduler().rootPool().addSchedulable(pool);
  }
  sqlc=getSQLContext();
  dep=getDependencyResolver();
  z=new ZeppelinContext(sc,sqlc,getHiveContext(),null,dep,printStream);
  try {
    if (sc.version().startsWith("1.1") || sc.version().startsWith("1.2")) {
      Method loadFiles=this.interpreter.getClass().getMethod("loadFiles",Settings.class);
      loadFiles.invoke(this.interpreter,settings);
    }
 else     if (sc.version().startsWith("1.3")) {
      Method loadFiles=this.interpreter.getClass().getMethod("org$apache$spark$repl$SparkILoop$$loadFiles",Settings.class);
      loadFiles.invoke(this.interpreter,settings);
    }
  }
 catch (  NoSuchMethodException|SecurityException|IllegalAccessException|IllegalArgumentException|InvocationTargetException e) {
    throw new InterpreterException(e);
  }
  intp.interpret("@transient var _binder = new java.util.HashMap[String, Object]()");
  binder=(Map<String,Object>)getValue("_binder");
  binder.put("sc",sc);
  binder.put("sqlc",sqlc);
  binder.put("hiveContext",getHiveContext());
  binder.put("z",z);
  binder.put("out",printStream);
  intp.interpret("@transient val z = " + "_binder.get(\"z\").asInstanceOf[com.nflabs.zeppelin.spark.ZeppelinContext]");
  intp.interpret("@transient val sc = " + "_binder.get(\"sc\").asInstanceOf[org.apache.spark.SparkContext]");
  intp.interpret("@transient val sqlc = " + "_binder.get(\"sqlc\").asInstanceOf[org.apache.spark.sql.SQLContext]");
  intp.interpret("@transient val hiveContext = " + "_binder.get(\"hiveContext\").asInstanceOf[org.apache.spark.sql.hive.HiveContext]");
  intp.interpret("import org.apache.spark.SparkContext._");
  if (sc.version().startsWith("1.1")) {
    intp.interpret("import sqlc._");
  }
 else   if (sc.version().startsWith("1.2")) {
    intp.interpret("import sqlc._");
  }
 else   if (sc.version().startsWith("1.3")) {
    intp.interpret("import sqlc.implicits._");
    intp.interpret("import sqlc.sql");
    intp.interpret("import org.apache.spark.sql.functions._");
  }
  if (depInterpreter != null) {
    DependencyContext depc=depInterpreter.getDependencyContext();
    if (depc != null) {
      List<File> files=depc.getFilesDist();
      if (files != null) {
        for (        File f : files) {
          if (f.getName().toLowerCase().endsWith(".jar")) {
            sc.addJar(f.getAbsolutePath());
            logger.info("sc.addJar(" + f.getAbsolutePath() + ")");
          }
 else {
            sc.addFile(f.getAbsolutePath());
            logger.info("sc.addFile(" + f.getAbsolutePath() + ")");
          }
        }
      }
    }
  }
}
