{
  if (!pythonscriptRunning) {
    return new InterpreterResult(Code.ERROR,"python process not running" + outputStream.toString());
  }
  outputStream.reset();
synchronized (pythonScriptInitializeNotifier) {
    long startTime=System.currentTimeMillis();
    while (pythonScriptInitialized == false && pythonscriptRunning && System.currentTimeMillis() - startTime < 10 * 1000) {
      try {
        pythonScriptInitializeNotifier.wait(1000);
      }
 catch (      InterruptedException e) {
      }
    }
  }
  if (pythonscriptRunning == false) {
    return new InterpreterResult(Code.ERROR,"failed to start pyspark" + outputStream.toString());
  }
  if (pythonScriptInitialized == false) {
    return new InterpreterResult(Code.ERROR,"pyspark is not responding " + outputStream.toString());
  }
  SparkInterpreter sparkInterpreter=getSparkInterpreter();
  if (!sparkInterpreter.getSparkContext().version().startsWith("1.2") && !sparkInterpreter.getSparkContext().version().startsWith("1.3") && !sparkInterpreter.getSparkContext().version().startsWith("1.4")) {
    return new InterpreterResult(Code.ERROR,"pyspark " + sparkInterpreter.getSparkContext().version() + " is not supported");
  }
  String jobGroup=sparkInterpreter.getJobGroup(context);
  ZeppelinContext z=sparkInterpreter.getZeppelinContext();
  z.setInterpreterContext(context);
  z.setGui(context.getGui());
  pythonInterpretRequest=new PythonInterpretRequest(st,jobGroup);
  statementOutput=null;
synchronized (statementSetNotifier) {
    statementSetNotifier.notify();
  }
synchronized (statementFinishedNotifier) {
    while (statementOutput == null) {
      try {
        statementFinishedNotifier.wait(1000);
      }
 catch (      InterruptedException e) {
      }
    }
  }
  if (statementError) {
    return new InterpreterResult(Code.ERROR,statementOutput);
  }
 else {
    return new InterpreterResult(Code.SUCCESS,statementOutput);
  }
}
