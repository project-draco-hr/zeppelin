{
  Note note=ZeppelinServer.notebook.createNote(null);
  int sparkVersionNumber=getSparkVersionNumber(note);
  if (isPyspark() && sparkVersionNumber >= 14) {
    List<InterpreterSetting> settings=ZeppelinServer.notebook.getBindedInterpreterSettings(note.id());
    for (    InterpreterSetting setting : settings) {
      if (setting.getName().equals("spark")) {
        ZeppelinServer.notebook.getInterpreterFactory().restart(setting.getId());
        break;
      }
    }
    Paragraph p0=note.addParagraph();
    Map config=p0.getConfig();
    config.put("enabled",true);
    p0.setConfig(config);
    p0.setText("%dep z.load(\"com.databricks:spark-csv_2.11:1.2.0\")");
    note.run(p0.getId());
    waitForFinish(p0);
    assertEquals(Status.FINISHED,p0.getStatus());
    File tmpFile=File.createTempFile("test","csv");
    FileUtils.write(tmpFile,"a,b\n1,2");
    Paragraph p1=note.addParagraph();
    p1.setConfig(config);
    String sqlContextName="sqlContext";
    if (sparkVersionNumber >= 20) {
      sqlContextName="spark";
    }
    p1.setText("%pyspark\n" + "from pyspark.sql import SQLContext\n" + "print(" + sqlContextName + ".read.format('com.databricks.spark.csv')"+ ".load('"+ tmpFile.getAbsolutePath()+ "').count())");
    note.run(p1.getId());
    waitForFinish(p1);
    assertEquals(Status.FINISHED,p1.getStatus());
    assertEquals("2\n",p1.getResult().message());
  }
}
