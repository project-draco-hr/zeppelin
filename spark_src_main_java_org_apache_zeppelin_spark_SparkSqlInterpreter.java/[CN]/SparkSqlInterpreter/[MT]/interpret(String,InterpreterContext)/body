{
  SQLContext sqlc=null;
  SparkInterpreter sparkInterpreter=getSparkInterpreter();
  if (sparkInterpreter.getSparkVersion().isUnsupportedVersion()) {
    return new InterpreterResult(Code.ERROR,"Spark " + sparkInterpreter.getSparkVersion().toString() + " is not supported");
  }
  sqlc=getSparkInterpreter().getSQLContext();
  SparkContext sc=sqlc.sparkContext();
  if (concurrentSQL()) {
    sc.setLocalProperty("spark.scheduler.pool","fair");
  }
 else {
    sc.setLocalProperty("spark.scheduler.pool",null);
  }
  sc.setJobGroup(getJobGroup(context),"Zeppelin",false);
  Object rdd=null;
  try {
    Method sqlMethod=sqlc.getClass().getMethod("sql",String.class);
    rdd=sqlMethod.invoke(sqlc,st);
  }
 catch (  InvocationTargetException ite) {
    if (Boolean.parseBoolean(getProperty("zeppelin.spark.sql.stacktrace"))) {
      throw new InterpreterException(ite);
    }
    logger.error("Invocation target exception",ite);
    String msg=ite.getTargetException().getMessage() + "\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace";
    return new InterpreterResult(Code.ERROR,msg);
  }
catch (  NoSuchMethodException|SecurityException|IllegalAccessException|IllegalArgumentException e) {
    throw new InterpreterException(e);
  }
  String msg=ZeppelinContext.showDF(sc,context,rdd,maxResult);
  sc.clearJobGroup();
  return new InterpreterResult(Code.SUCCESS,msg);
}
