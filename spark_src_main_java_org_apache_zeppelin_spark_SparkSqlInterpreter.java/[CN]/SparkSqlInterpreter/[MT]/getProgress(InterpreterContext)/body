{
  String jobGroup=getJobGroup(context);
  SQLContext sqlc=getSparkInterpreter().getSQLContext();
  SparkContext sc=sqlc.sparkContext();
  JobProgressListener sparkListener=getSparkInterpreter().getJobProgressListener();
  int completedTasks=0;
  int totalTasks=0;
  DAGScheduler scheduler=sc.dagScheduler();
  HashSet<ActiveJob> jobs=scheduler.activeJobs();
  Iterator<ActiveJob> it=jobs.iterator();
  while (it.hasNext()) {
    ActiveJob job=it.next();
    String g=(String)job.properties().get("spark.jobGroup.id");
    if (jobGroup.equals(g)) {
      int[] progressInfo=null;
      if (sc.version().startsWith("1.0")) {
        progressInfo=getProgressFromStage_1_0x(sparkListener,job.finalStage());
      }
 else       if (sc.version().startsWith("1.1")) {
        progressInfo=getProgressFromStage_1_1x(sparkListener,job.finalStage());
      }
 else       if (sc.version().startsWith("1.2")) {
        progressInfo=getProgressFromStage_1_1x(sparkListener,job.finalStage());
      }
 else       if (sc.version().startsWith("1.3")) {
        progressInfo=getProgressFromStage_1_1x(sparkListener,job.finalStage());
      }
 else       if (sc.version().startsWith("1.4")) {
        progressInfo=getProgressFromStage_1_1x(sparkListener,job.finalStage());
      }
 else {
        logger.warn("Spark {} getting progress information not supported" + sc.version());
        continue;
      }
      totalTasks+=progressInfo[0];
      completedTasks+=progressInfo[1];
    }
  }
  if (totalTasks == 0) {
    return 0;
  }
  return completedTasks * 100 / totalTasks;
}
