{
  startSessionState();
  conf.setBoolean("hive.test.init.phase",true);
  LinkedList<String> cols=new LinkedList<String>();
  cols.add("key");
  cols.add("value");
  LinkedList<String> part_cols=new LinkedList<String>();
  part_cols.add("ds");
  part_cols.add("hr");
  db.createTable("srcpart",cols,part_cols,TextInputFormat.class,IgnoreKeyTextOutputFormat.class);
  Path fpath;
  HashMap<String,String> part_spec=new HashMap<String,String>();
  for (  String ds : new String[]{"2008-04-08","2008-04-09"}) {
    for (    String hr : new String[]{"11","12"}) {
      part_spec.clear();
      part_spec.put("ds",ds);
      part_spec.put("hr",hr);
      fpath=new Path(testFiles,"kv1.txt");
      runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' OVERWRITE INTO TABLE srcpart PARTITION (ds='"+ ds+ "',hr='"+ hr+ "')");
    }
  }
  ArrayList<String> bucketCols=new ArrayList<String>();
  bucketCols.add("key");
  runCreateTableCmd("CREATE TABLE srcbucket(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE");
  for (  String fname : new String[]{"srcbucket0.txt","srcbucket1.txt"}) {
    fpath=new Path(testFiles,fname);
    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE srcbucket");
  }
  runCreateTableCmd("CREATE TABLE srcbucket2(key int, value string) " + "CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE");
  for (  String fname : new String[]{"srcbucket20.txt","srcbucket21.txt","srcbucket22.txt","srcbucket23.txt"}) {
    fpath=new Path(testFiles,fname);
    runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE srcbucket2");
  }
  for (  String tname : new String[]{"src","src1"}) {
    db.createTable(tname,cols,null,TextInputFormat.class,IgnoreKeyTextOutputFormat.class);
  }
  db.createTable("src_sequencefile",cols,null,SequenceFileInputFormat.class,SequenceFileOutputFormat.class);
  Table srcThrift=new Table(db.getCurrentDatabase(),"src_thrift");
  srcThrift.setInputFormatClass(SequenceFileInputFormat.class.getName());
  srcThrift.setOutputFormatClass(SequenceFileOutputFormat.class.getName());
  srcThrift.setSerializationLib(ThriftDeserializer.class.getName());
  srcThrift.setSerdeParam(Constants.SERIALIZATION_CLASS,Complex.class.getName());
  srcThrift.setSerdeParam(Constants.SERIALIZATION_FORMAT,TBinaryProtocol.class.getName());
  db.createTable(srcThrift);
  LinkedList<String> json_cols=new LinkedList<String>();
  json_cols.add("json");
  db.createTable("src_json",json_cols,null,TextInputFormat.class,IgnoreKeyTextOutputFormat.class);
  fpath=new Path(testFiles,"kv1.txt");
  runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src");
  fpath=new Path(testFiles,"kv3.txt");
  runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src1");
  fpath=new Path(testFiles,"kv1.seq");
  runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src_sequencefile");
  fpath=new Path(testFiles,"complex.seq");
  runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src_thrift");
  fpath=new Path(testFiles,"json.txt");
  runLoadCmd("LOAD DATA LOCAL INPATH '" + fpath.toString() + "' INTO TABLE src_json");
  conf.setBoolean("hive.test.init.phase",false);
}
