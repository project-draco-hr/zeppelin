{
  conf=new SparkConf();
  URL[] urls=getClassloaderUrls();
  Settings settings=new Settings();
  String args=getProperty("args");
  if (args == null) {
    args="";
  }
  String[] argsArray=args.split(" ");
  LinkedList<String> argList=new LinkedList<String>();
  for (  String arg : argsArray) {
    argList.add(arg);
  }
  if (Utils.isScala2_10()) {
    scala.collection.immutable.List<String> list=JavaConversions.asScalaBuffer(argList).toList();
    Object sparkCommandLine=Utils.instantiateClass("org.apache.spark.repl.SparkCommandLine",new Class[]{scala.collection.immutable.List.class},new Object[]{list});
    settings=(Settings)Utils.invokeMethod(sparkCommandLine,"settings");
  }
 else {
    String sparkReplClassDir=getProperty("spark.repl.classdir");
    if (sparkReplClassDir == null) {
      sparkReplClassDir=System.getProperty("spark.repl.classdir");
    }
    if (sparkReplClassDir == null) {
      sparkReplClassDir=System.getProperty("java.io.tmpdir");
    }
    outputDir=createTempDir(sparkReplClassDir);
    argList.add("-Yrepl-class-based");
    argList.add("-Yrepl-outdir");
    argList.add(outputDir.getAbsolutePath());
    scala.collection.immutable.List<String> list=JavaConversions.asScalaBuffer(argList).toList();
    settings.processArguments(list,true);
  }
  PathSetting pathSettings=settings.classpath();
  String classpath="";
  List<File> paths=currentClassPath();
  for (  File f : paths) {
    if (classpath.length() > 0) {
      classpath+=File.pathSeparator;
    }
    classpath+=f.getAbsolutePath();
  }
  if (urls != null) {
    for (    URL u : urls) {
      if (classpath.length() > 0) {
        classpath+=File.pathSeparator;
      }
      classpath+=u.getFile();
    }
  }
  DepInterpreter depInterpreter=getDepInterpreter();
  if (depInterpreter != null) {
    SparkDependencyContext depc=depInterpreter.getDependencyContext();
    if (depc != null) {
      List<File> files=depc.getFiles();
      if (files != null) {
        for (        File f : files) {
          if (classpath.length() > 0) {
            classpath+=File.pathSeparator;
          }
          classpath+=f.getAbsolutePath();
        }
      }
    }
  }
  String localRepo=getProperty("zeppelin.interpreter.localRepo");
  if (localRepo != null) {
    File localRepoDir=new File(localRepo);
    if (localRepoDir.exists()) {
      File[] files=localRepoDir.listFiles();
      if (files != null) {
        for (        File f : files) {
          if (classpath.length() > 0) {
            classpath+=File.pathSeparator;
          }
          classpath+=f.getAbsolutePath();
        }
      }
    }
  }
  pathSettings.v_$eq(classpath);
  settings.scala$tools$nsc$settings$ScalaSettings$_setter_$classpath_$eq(pathSettings);
  settings.explicitParentLoader_$eq(new Some<ClassLoader>(Thread.currentThread().getContextClassLoader()));
  BooleanSetting b=(BooleanSetting)settings.usejavacp();
  b.v_$eq(true);
  settings.scala$tools$nsc$settings$StandardScalaSettings$_setter_$usejavacp_$eq(b);
  System.setProperty("scala.repl.name.line","$line" + this.hashCode());
  MutableSettings.IntSetting numClassFileSetting=settings.maxClassfileName();
  numClassFileSetting.v_$eq(128);
  settings.scala$tools$nsc$settings$ScalaSettings$_setter_$maxClassfileName_$eq(numClassFileSetting);
synchronized (sharedInterpreterLock) {
    if (printREPLOutput()) {
      this.interpreter=new SparkILoop((java.io.BufferedReader)null,new PrintWriter(out));
    }
 else {
      this.interpreter=new SparkILoop((java.io.BufferedReader)null,new PrintWriter(Console.out(),false));
    }
    interpreter.settings_$eq(settings);
    interpreter.createInterpreter();
    intp=Utils.invokeMethod(interpreter,"intp");
    Utils.invokeMethod(intp,"setContextClassLoader");
    Utils.invokeMethod(intp,"initializeSynchronous");
    if (Utils.isScala2_10()) {
      if (classOutputDir == null) {
        classOutputDir=settings.outputDirs().getSingleOutput().get();
      }
 else {
        settings.outputDirs().setSingleOutput(classOutputDir);
        ClassLoader cl=(ClassLoader)Utils.invokeMethod(intp,"classLoader");
        try {
          Field rootField=cl.getClass().getSuperclass().getDeclaredField("root");
          rootField.setAccessible(true);
          rootField.set(cl,classOutputDir);
        }
 catch (        NoSuchFieldException|IllegalAccessException e) {
          logger.error(e.getMessage(),e);
        }
      }
      completor=Utils.instantiateClass("org.apache.spark.repl.SparkJLineCompletion",new Class[]{Utils.findClass("org.apache.spark.repl.SparkIMain")},new Object[]{intp});
    }
    if (Utils.isSpark2()) {
      sparkSession=getSparkSession();
    }
    sc=getSparkContext();
    if (sc.getPoolForName("fair").isEmpty()) {
      Value schedulingMode=org.apache.spark.scheduler.SchedulingMode.FAIR();
      int minimumShare=0;
      int weight=1;
      Pool pool=new Pool("fair",schedulingMode,minimumShare,weight);
      sc.taskScheduler().rootPool().addSchedulable(pool);
    }
    sparkVersion=SparkVersion.fromVersionString(sc.version());
    sqlc=getSQLContext();
    dep=getDependencyResolver();
    z=new ZeppelinContext(sc,sqlc,null,dep,Integer.parseInt(getProperty("zeppelin.spark.maxResult")));
    interpret("@transient val _binder = new java.util.HashMap[String, Object]()");
    Map<String,Object> binder;
    if (Utils.isScala2_10()) {
      binder=(Map<String,Object>)getValue("_binder");
    }
 else {
      binder=(Map<String,Object>)getLastObject();
    }
    binder.put("sc",sc);
    binder.put("sqlc",sqlc);
    binder.put("z",z);
    if (Utils.isSpark2()) {
      binder.put("spark",sparkSession);
    }
    interpret("@transient val z = " + "_binder.get(\"z\").asInstanceOf[org.apache.zeppelin.spark.ZeppelinContext]");
    interpret("@transient val sc = " + "_binder.get(\"sc\").asInstanceOf[org.apache.spark.SparkContext]");
    interpret("@transient val sqlc = " + "_binder.get(\"sqlc\").asInstanceOf[org.apache.spark.sql.SQLContext]");
    interpret("@transient val sqlContext = " + "_binder.get(\"sqlc\").asInstanceOf[org.apache.spark.sql.SQLContext]");
    if (Utils.isSpark2()) {
      interpret("@transient val spark = " + "_binder.get(\"spark\").asInstanceOf[org.apache.spark.sql.SparkSession]");
    }
    interpret("import org.apache.spark.SparkContext._");
    if (importImplicit()) {
      if (Utils.isSpark2()) {
        interpret("import spark.implicits._");
        interpret("import spark.sql");
        interpret("import org.apache.spark.sql.functions._");
      }
 else {
        if (sparkVersion.oldSqlContextImplicits()) {
          interpret("import sqlContext._");
        }
 else {
          interpret("import sqlContext.implicits._");
          interpret("import sqlContext.sql");
          interpret("import org.apache.spark.sql.functions._");
        }
      }
    }
  }
  if (Utils.isScala2_10()) {
    try {
      if (sparkVersion.oldLoadFilesMethodName()) {
        Method loadFiles=this.interpreter.getClass().getMethod("loadFiles",Settings.class);
        loadFiles.invoke(this.interpreter,settings);
      }
 else {
        Method loadFiles=this.interpreter.getClass().getMethod("org$apache$spark$repl$SparkILoop$$loadFiles",Settings.class);
        loadFiles.invoke(this.interpreter,settings);
      }
    }
 catch (    NoSuchMethodException|SecurityException|IllegalAccessException|IllegalArgumentException|InvocationTargetException e) {
      throw new InterpreterException(e);
    }
  }
  if (depInterpreter != null) {
    SparkDependencyContext depc=depInterpreter.getDependencyContext();
    if (depc != null) {
      List<File> files=depc.getFilesDist();
      if (files != null) {
        for (        File f : files) {
          if (f.getName().toLowerCase().endsWith(".jar")) {
            sc.addJar(f.getAbsolutePath());
            logger.info("sc.addJar(" + f.getAbsolutePath() + ")");
          }
 else {
            sc.addFile(f.getAbsolutePath());
            logger.info("sc.addFile(" + f.getAbsolutePath() + ")");
          }
        }
      }
    }
  }
  if (localRepo != null) {
    File localRepoDir=new File(localRepo);
    if (localRepoDir.exists()) {
      File[] files=localRepoDir.listFiles();
      if (files != null) {
        for (        File f : files) {
          if (f.getName().toLowerCase().endsWith(".jar")) {
            sc.addJar(f.getAbsolutePath());
            logger.info("sc.addJar(" + f.getAbsolutePath() + ")");
          }
 else {
            sc.addFile(f.getAbsolutePath());
            logger.info("sc.addFile(" + f.getAbsolutePath() + ")");
          }
        }
      }
    }
  }
  numReferenceOfSparkContext.incrementAndGet();
}
