{
  logger.info("------ Create new SparkContext {} -------",getProperty("master"));
  String execUri=System.getenv("SPARK_EXECUTOR_URI");
  String[] jars=SparkILoop.getAddedJars();
  String classServerUri=null;
  try {
    Method classServer=interpreter.intp().getClass().getMethod("classServer");
    HttpServer httpServer=(HttpServer)classServer.invoke(interpreter.intp());
    classServerUri=httpServer.uri();
  }
 catch (  NoSuchMethodException|SecurityException|IllegalAccessException|IllegalArgumentException|InvocationTargetException e) {
  }
  if (classServerUri == null) {
    try {
      Method classServer=interpreter.intp().getClass().getMethod("classServerUri");
      classServerUri=(String)classServer.invoke(interpreter.intp());
    }
 catch (    NoSuchMethodException|SecurityException|IllegalAccessException|IllegalArgumentException|InvocationTargetException e) {
      throw new InterpreterException(e);
    }
  }
  SparkConf conf=new SparkConf().setMaster(getProperty("master")).setAppName(getProperty("spark.app.name")).set("spark.repl.class.uri",classServerUri);
  if (jars.length > 0) {
    conf.setJars(jars);
  }
  if (execUri != null) {
    conf.set("spark.executor.uri",execUri);
  }
  if (System.getenv("SPARK_HOME") != null) {
    conf.setSparkHome(System.getenv("SPARK_HOME"));
  }
  conf.set("spark.scheduler.mode","FAIR");
  Properties intpProperty=getProperty();
  for (  Object k : intpProperty.keySet()) {
    String key=(String)k;
    String val=toString(intpProperty.get(key));
    if (!key.startsWith("spark.") || !val.trim().isEmpty()) {
      logger.debug(String.format("SparkConf: key = [%s], value = [%s]",key,val));
      conf.set(key,val);
    }
  }
  String pysparkBasePath=getSystemDefault("SPARK_HOME",null,null);
  File pysparkPath;
  if (null == pysparkBasePath) {
    pysparkBasePath=getSystemDefault("ZEPPELIN_HOME","zeppelin.home","../");
    pysparkPath=new File(pysparkBasePath,"interpreter" + File.separator + "spark"+ File.separator+ "pyspark");
  }
 else {
    pysparkPath=new File(pysparkBasePath,"python" + File.separator + "lib");
  }
  String[] pythonLibs=new String[]{"pyspark.zip","py4j-0.9-src.zip","py4j-0.8.2.1-src.zip"};
  ArrayList<String> pythonLibUris=new ArrayList<>();
  for (  String lib : pythonLibs) {
    File libFile=new File(pysparkPath,lib);
    if (libFile.exists()) {
      pythonLibUris.add(libFile.toURI().toString());
    }
  }
  pythonLibUris.trimToSize();
  if (pythonLibUris.size() == 2) {
    try {
      String confValue=conf.get("spark.yarn.dist.files");
      conf.set("spark.yarn.dist.files",confValue + "," + Joiner.on(",").join(pythonLibUris));
    }
 catch (    NoSuchElementException e) {
      conf.set("spark.yarn.dist.files",Joiner.on(",").join(pythonLibUris));
    }
    if (!useSparkSubmit()) {
      conf.set("spark.files",conf.get("spark.yarn.dist.files"));
    }
    conf.set("spark.submit.pyArchives",Joiner.on(":").join(pythonLibs));
  }
  if (getProperty("master").equals("yarn-client")) {
    conf.set("spark.yarn.isPython","true");
  }
  SparkContext sparkContext=new SparkContext(conf);
  return sparkContext;
}
